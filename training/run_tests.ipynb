{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import train_add_token_model\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from datasets import Dataset, load_metric, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import wandb\n",
    "import sys\n",
    "import train_sentence_classifier\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /tmp/tmpfwr6pyqi/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/tmp/tmpfwr6pyqi/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /tmp/tmpfwr6pyqi/added_tokens.json. We won't load it.\n",
      "loading file /tmp/tmpfwr6pyqi/vocab.json\n",
      "loading file /tmp/tmpfwr6pyqi/merges.txt\n",
      "loading file /tmp/tmpfwr6pyqi/tokenizer.json\n",
      "loading file None\n",
      "loading file /tmp/tmpfwr6pyqi/special_tokens_map.json\n",
      "loading file /tmp/tmpfwr6pyqi/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /tmp/tmpxacgxoal/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/tmp/tmpxacgxoal/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /tmp/tmpxacgxoal/added_tokens.json. We won't load it.\n",
      "loading file /tmp/tmpxacgxoal/vocab.json\n",
      "loading file /tmp/tmpxacgxoal/merges.txt\n",
      "loading file /tmp/tmpxacgxoal/tokenizer.json\n",
      "loading file None\n",
      "loading file /tmp/tmpxacgxoal/special_tokens_map.json\n",
      "loading file /tmp/tmpxacgxoal/tokenizer_config.json\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /home/ubuntu/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /home/ubuntu/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /home/ubuntu/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /home/ubuntu/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file ./output/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./output\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50267\n",
      "}\n",
      "\n",
      "loading weights file ./output/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at ./output.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "train, labels = train_add_token_model.load_data()\n",
    "dev, _ = train_add_token_model.load_data('data/dev.spacy')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "# For our custom tokens, let's add them\n",
    "tokenizer.add_tokens(['<PREAMBLE>', '<JUDGEMENT>'])\n",
    "\n",
    "model, trainer = train_add_token_model.create_model_and_trainer(train=train,\n",
    "                                              dev=dev,\n",
    "                                              all_labels=labels,\n",
    "                                              tokenizer=tokenizer,\n",
    "                                              batch_size=40,\n",
    "                                              epochs=40,\n",
    "                                              run_name='final_train',\n",
    "                                              pretrained='./output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4501/4501 [00:01<00:00, 2838.64ex/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'meta', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 4501\n",
       "})"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(row, idx):\n",
    "    text = row['text']\n",
    "    is_preamble = 'preamble' in row['meta']\n",
    "    if is_preamble:\n",
    "        text = '<PREAMBLE> ' + text\n",
    "    else:\n",
    "        text = '<JUDGEMENT> ' + text\n",
    "\n",
    "    return tokenizer(text, truncation=True, is_split_into_words=False)\n",
    "\n",
    "def load_test_data(tokenizer):\n",
    "    all_rows = []\n",
    "    for index, row in pd.read_json(\"../data/NER_TEST_DATA_FS.json\").iterrows():\n",
    "        all_rows.append({'text': row['data']['text'], 'meta': row['meta']['source']})\n",
    "\n",
    "    test = Dataset.from_list(all_rows)\n",
    "\n",
    "    return test.map(tokenize, with_indices=True)\n",
    "\n",
    "test = load_test_data(tokenizer)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: text, meta. If text, meta are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 4501\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[28, 28, 28, ..., 28, 28, 28],\n",
       "       [28, 28, 28, ..., 28, 28, 28],\n",
       "       [28, 28, 28, ..., 28, 28, 28],\n",
       "       ...,\n",
       "       [26, 28, 28, ...,  0,  0,  0],\n",
       "       [28, 28, 28, ...,  0,  0,  0],\n",
       "       [28, 28, 28, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(test)\n",
    "preds = np.argmax(preds[0], axis=2)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0f8e4fc0fdff428f993cf8507f3606e4',\n",
       " 'annotations': [{'result': ['a']}],\n",
       " 'data': {'text': 'In The High Court Of Kerala At Ernakulam\\n\\n                                       Present\\n\\n                    The Honourable Mrs. Justice M.R.Anitha\\n\\n           Wednesday, The 10Th Day Of June 2020 / 20Th Jyaishta, 1942\\n\\n                          Crl.Rev.Pet.No.767 Of 2012\\n\\n    Crmp 1176/2011 Dated 16-03-2012 Of Judicial Magistrate Of First Class ,\\n                                 Kunnamkulam\\n\\nRevision Petitioner/Complainant\\n                A. Rajesh Aged 35 Years,\\n                S/O.Raman Nair, Ammasom Veettil, Punnayurkulam Village, Cherayi\\n                Desom, Andathodu P.O., Chavakkad, Thrissur District, Pin\\n                679564.\\n                By Advs.\\n                Dr.V.N.Sankarjee\\n                Sri.V.N.Madhusudanan\\n                Sri.S.Sidhardhan\\n                Smt.R.Udaya Jyothi\\n                Sri.M.M.Vinod\\n                Smt.M.Suseela\\n                Sri.Sudhakaran V.\\n                Smt.Arya Balachandran\\n                Smt. Keerthi B. Chandran\\n\\nRespondents/State And Accused 1 To 6\\n\\n       1        State Of Kerala\\n                Represented By The Public Prosecutor, High Court Of Kerala,\\n                Ernakulam.\\n       2        Sajin Sasi\\n                Sub Inspector Of Police, Vadakkekkad, Thrissur District, Pin\\n                679564.\\n\\n       3        Surendran\\n                Police Constable, Vadakkekad, Pin 679564.\\n\\n       4        Joshi,Assistant Sub Inspector Of Police, Vadakkekkad, Pin\\n                679564.\\n\\n       5        Dinesan, Police Constable, Vadakkekad, Pin 679564.\\n\\n\\n\\n       6        Stephen\\n                Police Constable, Vadakkekad, Pin 679564.\\n\\n                By Adv. Sri.K.K.Dheerendrakrishnan\\n                By Adv. Sri.S.Rajeev\\n                Sr.Pp- Sri. M.S.Breez\\n\\n      This Criminal Revision Petition Having Been Finally Heard On 26.5.2020,\\nThe Court On 10.06.2020, The Court On The Same Day Passed The Following:\\n Crl.R.P.767/2012\\n                                            2\\n\\n                                 Order\\n'},\n",
       " 'meta': {'source': 'criminal_kerala_high_court preamble https://indiankanoon.org/doc/122996227/'}}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file = open('../data/NER_TEST_DATA_FS.json')\n",
    "data = json.load(file)\n",
    "data[0]['annotations'] = [{'result': ['a']}]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4501/4501 [00:02<00:00, 1657.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sent_index in tqdm(range(len(data))):\n",
    "    annotations = []\n",
    "    pred = preds[sent_index]\n",
    "    \n",
    "    original_text = test[sent_index]['text']\n",
    "    # We have a list of preds, we need to match them up with tokens and find the original character range\n",
    "    # Retokenize the text so we can figure out what words correspond to what char ranges\n",
    "    # If we had included the class token, it would be tougher to align the char indices with the original text\n",
    "    sent_tokenized = tokenizer(original_text, truncation=True, is_split_into_words=False)\n",
    "    \n",
    "    # We may need to keep track of a label over multiple tokens\n",
    "    current_label = None # This will be a tuple (label name, start index, end index)\n",
    "    \n",
    "    for token_index in range(1, len(sent_tokenized['input_ids'])-1):\n",
    "        # Iterate through each token in the sentence (skip the first)\n",
    "        tag = labels[pred[token_index + 1]] # We are off by one because our predictions include a prediction for the class token\n",
    "        \n",
    "        token_indices = sent_tokenized.token_to_chars(token_index)\n",
    "        start_index = token_indices.start\n",
    "        end_index = token_indices.end\n",
    "        \n",
    "        if 'I' in tag:\n",
    "            # We must be following a B tag or we made an error\n",
    "            # So there should be a current_label\n",
    "            if current_label:\n",
    "                current_label = (current_label[0], current_label[1], end_index)\n",
    "        elif current_label:\n",
    "            # If we previously were tracking a label, we need to end it, since we are now looking at a B or O tag\n",
    "            annotations.append({'value': {'start': current_label[1], \n",
    "                                          'end': current_label[2], \n",
    "                                          'text': original_text[current_label[1]: current_label[2]],\n",
    "                                          'labels': [current_label[0]]},\n",
    "                                'id': f\"{sent_index}{token_index}\",\n",
    "                                'from_name': 'label',\n",
    "                                'to_name': 'label',\n",
    "                                'type': 'labels'\n",
    "                               })\n",
    "            current_label = None\n",
    "        \n",
    "        if 'B' in tag:\n",
    "            current_label = (tag[2:], start_index, end_index)\n",
    "    \n",
    "    data[sent_index]['annotations'] = [{'result': annotations}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NER_TEST_DATA_FS.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>'] O\n",
      "['<PREAMBLE>'] O\n",
      "['ĠIn'] O\n",
      "['ĠThe'] O\n",
      "['ĠHigh'] B-COURT\n",
      "['ĠCourt'] I-COURT\n",
      "['ĠOf'] I-COURT\n",
      "['ĠKerala'] I-COURT\n",
      "['ĠAt'] I-COURT\n",
      "['ĠErn'] I-COURT\n",
      "['ak'] I-COURT\n",
      "['ul'] I-COURT\n",
      "['am'] I-COURT\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠPresent'] O\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠThe'] O\n",
      "['ĠHonour'] O\n",
      "['able'] O\n",
      "['ĠMrs'] O\n",
      "['.'] O\n",
      "['ĠJustice'] O\n",
      "['ĠM'] B-JUDGE\n",
      "['.'] I-JUDGE\n",
      "['R'] I-JUDGE\n",
      "['.'] I-JUDGE\n",
      "['An'] I-JUDGE\n",
      "['ith'] I-JUDGE\n",
      "['a'] I-JUDGE\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠWednesday'] O\n",
      "[','] O\n",
      "['ĠThe'] O\n",
      "['Ġ10'] O\n",
      "['Th'] O\n",
      "['ĠDay'] O\n",
      "['ĠOf'] O\n",
      "['ĠJune'] O\n",
      "['Ġ2020'] O\n",
      "['Ġ/'] O\n",
      "['Ġ20'] O\n",
      "['Th'] O\n",
      "['ĠJ'] O\n",
      "['ya'] O\n",
      "['ish'] O\n",
      "['ta'] O\n",
      "[','] O\n",
      "['Ġ1942'] O\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠCr'] O\n",
      "['l'] O\n",
      "['.'] O\n",
      "['Rev'] O\n",
      "['.'] O\n",
      "['Pet'] O\n",
      "['.'] O\n",
      "['No'] O\n",
      "['.'] O\n",
      "['767'] O\n",
      "['ĠOf'] O\n",
      "['Ġ2012'] O\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠCr'] O\n",
      "['mp'] O\n",
      "['Ġ11'] O\n",
      "['76'] O\n",
      "['/'] O\n",
      "['2011'] O\n",
      "['ĠD'] O\n",
      "['ated'] O\n",
      "['Ġ16'] O\n",
      "['-'] O\n",
      "['03'] O\n",
      "['-'] O\n",
      "['2012'] O\n",
      "['ĠOf'] O\n",
      "['ĠJudicial'] O\n",
      "['ĠMag'] O\n",
      "['istrate'] O\n",
      "['ĠOf'] O\n",
      "['ĠFirst'] O\n",
      "['ĠClass'] O\n",
      "['Ġ,'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠKun'] O\n",
      "['nam'] O\n",
      "['k'] O\n",
      "['ul'] O\n",
      "['am'] O\n",
      "['Ċ'] O\n",
      "['Ċ'] O\n",
      "['Rev'] O\n",
      "['ision'] O\n",
      "['ĠPetition'] O\n",
      "['er'] O\n",
      "['/'] O\n",
      "['Com'] O\n",
      "['plain'] O\n",
      "['ant'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠA'] B-PETITIONER\n",
      "['.'] I-PETITIONER\n",
      "['ĠRaj'] I-PETITIONER\n",
      "['esh'] I-PETITIONER\n",
      "['ĠA'] O\n",
      "['ged'] O\n",
      "['Ġ35'] O\n",
      "['ĠYears'] O\n",
      "[','] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠS'] O\n",
      "['/'] O\n",
      "['O'] O\n",
      "['.'] O\n",
      "['R'] O\n",
      "['aman'] O\n",
      "['ĠN'] O\n",
      "['air'] O\n",
      "[','] O\n",
      "['ĠAm'] O\n",
      "['mas'] O\n",
      "['om'] O\n",
      "['ĠVe'] O\n",
      "['ett'] O\n",
      "['il'] O\n",
      "[','] O\n",
      "['ĠPun'] O\n",
      "['n'] O\n",
      "['ay'] O\n",
      "['ur'] O\n",
      "['k'] O\n",
      "['ul'] O\n",
      "['am'] O\n",
      "['ĠVillage'] O\n",
      "[','] O\n",
      "['ĠCher'] O\n",
      "['ay'] O\n",
      "['i'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠDes'] O\n",
      "['om'] O\n",
      "[','] O\n",
      "['ĠAnd'] O\n",
      "['ath'] O\n",
      "['od'] O\n",
      "['u'] O\n",
      "['ĠP'] O\n",
      "['.'] O\n",
      "['O'] O\n",
      "['.,'] O\n",
      "['ĠCh'] O\n",
      "['av'] O\n",
      "['ak'] O\n",
      "['k'] O\n",
      "['ad'] O\n",
      "[','] O\n",
      "['ĠThr'] O\n",
      "['iss'] O\n",
      "['ur'] O\n",
      "['ĠDistrict'] O\n",
      "[','] O\n",
      "['ĠPin'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ6'] O\n",
      "['795'] O\n",
      "['64'] O\n",
      "['.'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠBy'] O\n",
      "['ĠAdv'] O\n",
      "['s'] O\n",
      "['.'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠDr'] O\n",
      "['.'] O\n",
      "['V'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['N'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['S'] I-LAWYER\n",
      "['ank'] I-LAWYER\n",
      "['ar'] I-LAWYER\n",
      "['jee'] I-LAWYER\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSri'] B-LAWYER\n",
      "['.'] B-LAWYER\n",
      "['V'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['N'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['Mad'] I-LAWYER\n",
      "['hus'] I-LAWYER\n",
      "['ud'] I-LAWYER\n",
      "['anan'] I-LAWYER\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSri'] B-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['S'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['S'] I-LAWYER\n",
      "['id'] I-LAWYER\n",
      "['hard'] I-LAWYER\n",
      "['han'] I-LAWYER\n",
      "['Ċ'] I-LAWYER\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSm'] O\n",
      "['t'] O\n",
      "['.'] O\n",
      "['R'] B-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['U'] I-LAWYER\n",
      "['day'] I-LAWYER\n",
      "['a'] I-LAWYER\n",
      "['ĠJ'] I-LAWYER\n",
      "['y'] I-LAWYER\n",
      "['oth'] I-LAWYER\n",
      "['i'] I-LAWYER\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSri'] B-LAWYER\n",
      "['.'] B-LAWYER\n",
      "['M'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['M'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['V'] I-LAWYER\n",
      "['in'] I-LAWYER\n",
      "['od'] I-LAWYER\n",
      "['Ċ'] I-LAWYER\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['</s>'] O\n"
     ]
    }
   ],
   "source": [
    "for (token, label) in zip(test[0]['input_ids'], preds[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens([token]), labels[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>'] O O\n",
      "['<PREAMBLE>'] O O\n",
      "['Ġ'] O O\n",
      "['ĊĊ'] O O\n",
      "['Ġ('] O O\n",
      "['Ġ7'] O O\n",
      "['Ġ)'] O O\n",
      "['ĠOn'] O O\n",
      "['Ġspecific'] O O\n",
      "['Ġquery'] O O\n",
      "['Ġby'] O O\n",
      "['Ġthe'] O O\n",
      "['ĠBench'] O O\n",
      "['Ġabout'] O O\n",
      "['Ġan'] O O\n",
      "['Ġentry'] O O\n",
      "['Ġof'] O O\n",
      "['ĠRs'] O O\n",
      "['Ġ.'] O O\n",
      "['Ġ1'] O O\n",
      "[','] O O\n",
      "['31'] O O\n",
      "[','] O O\n",
      "['37'] O O\n",
      "[','] O O\n",
      "['500'] O O\n",
      "['Ġon'] O O\n",
      "['Ġdeposit'] O O\n",
      "['Ġside'] O O\n",
      "['Ġof'] O O\n",
      "['ĠHong'] B-ORG B-ORG\n",
      "['k'] B-ORG I-ORG\n",
      "['ong'] B-ORG I-ORG\n",
      "['ĠBank'] I-ORG I-ORG\n",
      "['Ġaccount'] O O\n",
      "['Ġof'] O O\n",
      "['Ġwhich'] O O\n",
      "['Ġa'] O O\n",
      "['Ġphoto'] O O\n",
      "['Ġcopy'] O O\n",
      "['Ġis'] O O\n",
      "['Ġappearing'] O O\n",
      "['Ġat'] O O\n",
      "['Ġp'] O O\n",
      "['.'] O O\n",
      "['Ġ40'] O O\n",
      "['Ġof'] O O\n",
      "['Ġass'] O O\n",
      "['essee'] O O\n",
      "[\"Ġ'\"] O O\n",
      "['s'] O O\n",
      "['Ġpaper'] O O\n",
      "['Ġbook'] O O\n",
      "['Ġ,'] O O\n",
      "['Ġlearned'] O O\n",
      "['Ġauthorised'] O O\n",
      "['Ġrepresentative'] O O\n",
      "['Ġsubmitted'] O O\n",
      "['Ġthat'] O O\n",
      "['Ġit'] O O\n",
      "['Ġwas'] O O\n",
      "['Ġrelated'] O O\n",
      "['Ġto'] O O\n",
      "['Ġloan'] O O\n",
      "['Ġfrom'] O O\n",
      "['Ġbroker'] O O\n",
      "['Ġ,'] O O\n",
      "['ĠRahul'] B-ORG B-ORG\n",
      "['Ġ&'] I-ORG I-ORG\n",
      "['ĠCo'] I-ORG I-ORG\n",
      "['.'] I-ORG I-ORG\n",
      "['Ġon'] O O\n",
      "['Ġthe'] O O\n",
      "['Ġbasis'] O O\n",
      "['Ġof'] O O\n",
      "['Ġhis'] O O\n",
      "['Ġsubmission'] O O\n",
      "['Ġa'] O O\n",
      "['Ġnecessary'] O O\n",
      "['Ġmark'] O O\n",
      "['Ġis'] O O\n",
      "['Ġput'] O O\n",
      "['Ġby'] O O\n",
      "['Ġus'] O O\n",
      "['Ġon'] O O\n",
      "['Ġthat'] O O\n",
      "['Ġphoto'] O O\n",
      "['Ġcopy'] O O\n",
      "['Ġ.'] O O\n",
      "['</s>'] O O\n"
     ]
    }
   ],
   "source": [
    "def train_tokenize(row, idx):\n",
    "    # Add special token for document type\n",
    "    is_preamble = True\n",
    "    if is_preamble:\n",
    "        row['tokens'].insert(0, '<PREAMBLE>')\n",
    "    else:\n",
    "        row['tokens'].insert(0, '<JUDGEMENT>')\n",
    "    row['tags'].insert(0, 'O')\n",
    "\n",
    "    tokenized = tokenizer(row['tokens'], truncation=True, is_split_into_words=True)\n",
    "    aligned_labels = []\n",
    "    last_i = None\n",
    "    for i in tokenized.word_ids():\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "            continue\n",
    "        \n",
    "        aligned_label = row['tags'][i] # Find the appropriate label index\n",
    "        if not i == last_i:\n",
    "            aligned_labels.append(labels.index(aligned_label))\n",
    "        else:\n",
    "            aligned_labels.append(labels.index(aligned_label.replace('B', 'I')))\n",
    "        last_i = i\n",
    "    tokenized['labels'] = aligned_labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_tokenize(train[0], 0)\n",
    "train_preds = np.argmax(trainer.predict([train_tokenized])[0], axis=2)[0]\n",
    "for (token, label, real) in zip(train_tokenized['input_ids'], train_preds, train_tokenized['labels']):\n",
    "    print(tokenizer.convert_ids_to_tokens([token]), labels[label], labels[real if real >= 0 else 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Wa-305', '-', '2007', '\\n         ', '(', 'Purushottam', 'Lal', 'Vs', 'The', 'State', 'Of', 'Madhya', 'Pradesh', ')', '\\n\\n\\n', '15', '-', '10', '-', '2015', '\\n    ', 'High', 'Court', 'Of', 'Madhya', 'Pradesh', 'Principal', '\\n              ', 'Seat', 'At', 'Jabalpur', '\\n                 ', 'Writ', 'Appeal', 'No.305/2007', '\\n                 ', 'Purushottam', 'Lal', 'and', 'others', '\\n                             ', 'Vs', '.', '\\n                   ', 'State', 'of', 'M.P.', '&', 'Others', '\\n', 'Present', ':', 'Honâ\\x80\\x99ble', 'Shri', 'Rajendra', 'Menon', ',', 'J.', '&', '\\n', \"Hon'ble\", 'Shri', 'C.', 'V.', 'Sirpurkar', ',', 'J.', '\\n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '\\n', 'Shri', 'Vivek', 'Tankha', ',', 'learned', 'Senior', 'Counsel', 'with', 'Shri', 'Varun', 'K.', '\\n', 'Chopra', ',', 'Shri', 'Akshay', 'Sapre', ',', 'for', 'the', 'appellants', '.', '\\n', 'Shri', 'Swapnil', 'Ganguly', ',', 'learned', 'Govt', '.', 'Adv', '.', ',', 'for', 'the', 'respondents', '\\n', 'State', '.', '\\n', 'Ku', '.', 'Anjali', 'Banerjee', ',', 'learned', 'counsel', 'for', 'M.P.', 'Housing', 'Board', '.', '\\n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '\\n                       ', 'Judgment', '\\n'], 'tags': ['O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(train[-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom",
   "language": "python",
   "name": "custom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
