{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import train_add_token_model\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from datasets import Dataset, load_metric, concatenate_datasets\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import wandb\n",
    "import sys\n",
    "import train_sentence_classifier\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading data...\n",
      "Creating model...\n"
     ]
    }
   ],
   "source": [
    "train, labels = train_add_token_model.load_data()\n",
    "dev, _ = train_add_token_model.load_data('data/dev.spacy')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base', add_prefix_space=True)\n",
    "# For our custom tokens, let's add them\n",
    "tokenizer.add_tokens(['<PREAMBLE>', '<JUDGEMENT>'])\n",
    "\n",
    "model, trainer = train_add_token_model.create_model_and_trainer(train=train,\n",
    "                                              dev=dev,\n",
    "                                              all_labels=labels,\n",
    "                                              tokenizer=tokenizer,\n",
    "                                              batch_size=40,\n",
    "                                              epochs=40,\n",
    "                                              run_name='final_train',\n",
    "                                              pretrained='./output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e61b93064cf14de1837c19e9ad5f0ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'meta', 'id', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5037\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    processed_batch = {'input_ids': [], 'attention_mask': [], 'id': [], 'text': [], 'meta': []}\n",
    "\n",
    "    for text, meta, row_id in zip(batch['text'], batch['meta'], batch['id']):\n",
    "        is_preamble = 'preamble' in meta\n",
    "        if is_preamble:\n",
    "            text = '<PREAMBLE> ' + text\n",
    "        else:\n",
    "            text = '<JUDGEMENT> ' + text\n",
    "        tokenized = tokenizer(text, truncation=False, is_split_into_words=False)\n",
    "        \n",
    "        for token_index in range(0, len(tokenized['input_ids']), 512):\n",
    "            processed_batch['input_ids'].append(tokenized['input_ids'][token_index:token_index+512])\n",
    "            processed_batch['attention_mask'].append(tokenized['attention_mask'][token_index:token_index+512])\n",
    "            processed_batch['id'].append(row_id)\n",
    "            processed_batch['text'].append(text)\n",
    "            processed_batch['meta'].append(meta)\n",
    "    return processed_batch\n",
    "\n",
    "def load_test_data(tokenizer):\n",
    "    all_rows = []\n",
    "    for index, row in pd.read_json(\"../data/NER_TEST_DATA_FS.json\").iterrows():\n",
    "        all_rows.append({'text': row['data']['text'], 'meta': row['meta']['source'], 'id': row['id']})\n",
    "\n",
    "    test = Dataset.from_list(all_rows)\n",
    "\n",
    "    return test.map(tokenize, batched=True)\n",
    "\n",
    "test = load_test_data(tokenizer)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: meta, text, id. If meta, text, id are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5037\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[28, 28, 28, ..., 28, 28, 28],\n",
       "       [28, 28, 28, ..., 28, 28, 28],\n",
       "       [28,  7, 21, ..., 28, 28, 28],\n",
       "       ...,\n",
       "       [26, 28, 28, ...,  0,  0,  0],\n",
       "       [28, 28, 28, ...,  0,  0,  0],\n",
       "       [28, 28, 28, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(test)\n",
    "preds = np.argmax(preds[0], axis=2)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0f8e4fc0fdff428f993cf8507f3606e4',\n",
       " 'annotations': [{'result': ['a']}],\n",
       " 'data': {'text': 'In The High Court Of Kerala At Ernakulam\\n\\n                                       Present\\n\\n                    The Honourable Mrs. Justice M.R.Anitha\\n\\n           Wednesday, The 10Th Day Of June 2020 / 20Th Jyaishta, 1942\\n\\n                          Crl.Rev.Pet.No.767 Of 2012\\n\\n    Crmp 1176/2011 Dated 16-03-2012 Of Judicial Magistrate Of First Class ,\\n                                 Kunnamkulam\\n\\nRevision Petitioner/Complainant\\n                A. Rajesh Aged 35 Years,\\n                S/O.Raman Nair, Ammasom Veettil, Punnayurkulam Village, Cherayi\\n                Desom, Andathodu P.O., Chavakkad, Thrissur District, Pin\\n                679564.\\n                By Advs.\\n                Dr.V.N.Sankarjee\\n                Sri.V.N.Madhusudanan\\n                Sri.S.Sidhardhan\\n                Smt.R.Udaya Jyothi\\n                Sri.M.M.Vinod\\n                Smt.M.Suseela\\n                Sri.Sudhakaran V.\\n                Smt.Arya Balachandran\\n                Smt. Keerthi B. Chandran\\n\\nRespondents/State And Accused 1 To 6\\n\\n       1        State Of Kerala\\n                Represented By The Public Prosecutor, High Court Of Kerala,\\n                Ernakulam.\\n       2        Sajin Sasi\\n                Sub Inspector Of Police, Vadakkekkad, Thrissur District, Pin\\n                679564.\\n\\n       3        Surendran\\n                Police Constable, Vadakkekad, Pin 679564.\\n\\n       4        Joshi,Assistant Sub Inspector Of Police, Vadakkekkad, Pin\\n                679564.\\n\\n       5        Dinesan, Police Constable, Vadakkekad, Pin 679564.\\n\\n\\n\\n       6        Stephen\\n                Police Constable, Vadakkekad, Pin 679564.\\n\\n                By Adv. Sri.K.K.Dheerendrakrishnan\\n                By Adv. Sri.S.Rajeev\\n                Sr.Pp- Sri. M.S.Breez\\n\\n      This Criminal Revision Petition Having Been Finally Heard On 26.5.2020,\\nThe Court On 10.06.2020, The Court On The Same Day Passed The Following:\\n Crl.R.P.767/2012\\n                                            2\\n\\n                                 Order\\n'},\n",
       " 'meta': {'source': 'criminal_kerala_high_court preamble https://indiankanoon.org/doc/122996227/'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "file = open('../data/NER_TEST_DATA_FS.json')\n",
    "data = json.load(file)\n",
    "data[0]['annotations'] = [{'result': ['a']}]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 4501/4501 [40:16<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sent_index in tqdm(range(len(data))):\n",
    "    annotations = []\n",
    "    \n",
    "    # Find all the matching predictions, which may come from multiple rows (if the sentence was split up)\n",
    "    sent_id = data[sent_index]['id']\n",
    "    \n",
    "    pred = []\n",
    "    for input_index in range(len(test)):\n",
    "        if test[input_index]['id'] == sent_id:\n",
    "            pred += list(preds[input_index])\n",
    "    \n",
    "    \n",
    "    original_text = data[sent_index]['data']['text']\n",
    "    # We have a list of preds, we need to match them up with tokens and find the original character range\n",
    "    # Retokenize the text so we can figure out what words correspond to what char ranges\n",
    "    # If we had included the class token, it would be tougher to align the char indices with the original text\n",
    "    sent_tokenized = tokenizer(original_text, truncation=False, is_split_into_words=False)\n",
    "    \n",
    "    # We may need to keep track of a label over multiple tokens\n",
    "    current_label = None # This will be a tuple (label name, start index, end index)\n",
    "    \n",
    "    for token_index in range(1, len(sent_tokenized['input_ids'])-1):\n",
    "        # Iterate through each token in the sentence (skip the first)\n",
    "        tag = labels[pred[token_index + 1]] # We are off by one because our predictions include a prediction for the class token\n",
    "        \n",
    "        token_indices = sent_tokenized.token_to_chars(token_index)\n",
    "        start_index = token_indices.start\n",
    "        end_index = token_indices.end\n",
    "        \n",
    "        if 'I' in tag:\n",
    "            # We must be following a B tag or we made an error\n",
    "            # So there should be a current_label\n",
    "            if current_label:\n",
    "                current_label = (current_label[0], current_label[1], end_index)\n",
    "        elif current_label:\n",
    "            # If we previously were tracking a label, we need to end it, since we are now looking at a B or O tag\n",
    "            annotations.append({'value': {'start': current_label[1], \n",
    "                                          'end': current_label[2], \n",
    "                                          'text': original_text[current_label[1]: current_label[2]],\n",
    "                                          'labels': [current_label[0]]},\n",
    "                                'id': f\"{sent_index}{token_index}\",\n",
    "                                'from_name': 'label',\n",
    "                                'to_name': 'label',\n",
    "                                'type': 'labels'\n",
    "                               })\n",
    "            current_label = None\n",
    "        \n",
    "        if 'B' in tag:\n",
    "            current_label = (tag[2:], start_index, end_index)\n",
    "    \n",
    "    data[sent_index]['annotations'] = [{'result': annotations}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NER_TEST_DATA_FS.json\", \"w\") as outfile:\n",
    "    outfile.write(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>'] O\n",
      "['<PREAMBLE>'] O\n",
      "['ĠIn'] O\n",
      "['ĠThe'] O\n",
      "['ĠHigh'] B-COURT\n",
      "['ĠCourt'] I-COURT\n",
      "['ĠOf'] I-COURT\n",
      "['ĠKerala'] I-COURT\n",
      "['ĠAt'] I-COURT\n",
      "['ĠErn'] I-COURT\n",
      "['ak'] I-COURT\n",
      "['ul'] I-COURT\n",
      "['am'] I-COURT\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠPresent'] O\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠThe'] O\n",
      "['ĠHonour'] O\n",
      "['able'] O\n",
      "['ĠMrs'] O\n",
      "['.'] O\n",
      "['ĠJustice'] O\n",
      "['ĠM'] B-JUDGE\n",
      "['.'] I-JUDGE\n",
      "['R'] I-JUDGE\n",
      "['.'] I-JUDGE\n",
      "['An'] I-JUDGE\n",
      "['ith'] I-JUDGE\n",
      "['a'] I-JUDGE\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠWednesday'] O\n",
      "[','] O\n",
      "['ĠThe'] O\n",
      "['Ġ10'] O\n",
      "['Th'] O\n",
      "['ĠDay'] O\n",
      "['ĠOf'] O\n",
      "['ĠJune'] O\n",
      "['Ġ2020'] O\n",
      "['Ġ/'] O\n",
      "['Ġ20'] O\n",
      "['Th'] O\n",
      "['ĠJ'] O\n",
      "['ya'] O\n",
      "['ish'] O\n",
      "['ta'] O\n",
      "[','] O\n",
      "['Ġ1942'] O\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠCr'] O\n",
      "['l'] O\n",
      "['.'] O\n",
      "['Rev'] O\n",
      "['.'] O\n",
      "['Pet'] O\n",
      "['.'] O\n",
      "['No'] O\n",
      "['.'] O\n",
      "['767'] O\n",
      "['ĠOf'] O\n",
      "['Ġ2012'] O\n",
      "['ĊĊ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠCr'] O\n",
      "['mp'] O\n",
      "['Ġ11'] O\n",
      "['76'] O\n",
      "['/'] O\n",
      "['2011'] O\n",
      "['ĠD'] O\n",
      "['ated'] O\n",
      "['Ġ16'] O\n",
      "['-'] O\n",
      "['03'] O\n",
      "['-'] O\n",
      "['2012'] O\n",
      "['ĠOf'] O\n",
      "['ĠJudicial'] O\n",
      "['ĠMag'] O\n",
      "['istrate'] O\n",
      "['ĠOf'] O\n",
      "['ĠFirst'] O\n",
      "['ĠClass'] O\n",
      "['Ġ,'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠKun'] O\n",
      "['nam'] O\n",
      "['k'] O\n",
      "['ul'] O\n",
      "['am'] O\n",
      "['Ċ'] O\n",
      "['Ċ'] O\n",
      "['Rev'] O\n",
      "['ision'] O\n",
      "['ĠPetition'] O\n",
      "['er'] O\n",
      "['/'] O\n",
      "['Com'] O\n",
      "['plain'] O\n",
      "['ant'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠA'] B-PETITIONER\n",
      "['.'] I-PETITIONER\n",
      "['ĠRaj'] I-PETITIONER\n",
      "['esh'] I-PETITIONER\n",
      "['ĠA'] O\n",
      "['ged'] O\n",
      "['Ġ35'] O\n",
      "['ĠYears'] O\n",
      "[','] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠS'] O\n",
      "['/'] O\n",
      "['O'] O\n",
      "['.'] O\n",
      "['R'] O\n",
      "['aman'] O\n",
      "['ĠN'] O\n",
      "['air'] O\n",
      "[','] O\n",
      "['ĠAm'] O\n",
      "['mas'] O\n",
      "['om'] O\n",
      "['ĠVe'] O\n",
      "['ett'] O\n",
      "['il'] O\n",
      "[','] O\n",
      "['ĠPun'] O\n",
      "['n'] O\n",
      "['ay'] O\n",
      "['ur'] O\n",
      "['k'] O\n",
      "['ul'] O\n",
      "['am'] O\n",
      "['ĠVillage'] O\n",
      "[','] O\n",
      "['ĠCher'] O\n",
      "['ay'] O\n",
      "['i'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠDes'] O\n",
      "['om'] O\n",
      "[','] O\n",
      "['ĠAnd'] O\n",
      "['ath'] O\n",
      "['od'] O\n",
      "['u'] O\n",
      "['ĠP'] O\n",
      "['.'] O\n",
      "['O'] O\n",
      "['.,'] O\n",
      "['ĠCh'] O\n",
      "['av'] O\n",
      "['ak'] O\n",
      "['k'] O\n",
      "['ad'] O\n",
      "[','] O\n",
      "['ĠThr'] O\n",
      "['iss'] O\n",
      "['ur'] O\n",
      "['ĠDistrict'] O\n",
      "[','] O\n",
      "['ĠPin'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ6'] O\n",
      "['795'] O\n",
      "['64'] O\n",
      "['.'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠBy'] O\n",
      "['ĠAdv'] O\n",
      "['s'] O\n",
      "['.'] O\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠDr'] O\n",
      "['.'] O\n",
      "['V'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['N'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['S'] I-LAWYER\n",
      "['ank'] I-LAWYER\n",
      "['ar'] I-LAWYER\n",
      "['jee'] I-LAWYER\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSri'] B-LAWYER\n",
      "['.'] B-LAWYER\n",
      "['V'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['N'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['Mad'] I-LAWYER\n",
      "['hus'] I-LAWYER\n",
      "['ud'] I-LAWYER\n",
      "['anan'] I-LAWYER\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSri'] B-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['S'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['S'] I-LAWYER\n",
      "['id'] I-LAWYER\n",
      "['hard'] I-LAWYER\n",
      "['han'] I-LAWYER\n",
      "['Ċ'] I-LAWYER\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSm'] O\n",
      "['t'] O\n",
      "['.'] O\n",
      "['R'] B-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['U'] I-LAWYER\n",
      "['day'] I-LAWYER\n",
      "['a'] I-LAWYER\n",
      "['ĠJ'] I-LAWYER\n",
      "['y'] I-LAWYER\n",
      "['oth'] I-LAWYER\n",
      "['i'] I-LAWYER\n",
      "['Ċ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['ĠSri'] B-LAWYER\n",
      "['.'] B-LAWYER\n",
      "['M'] B-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['M'] I-LAWYER\n",
      "['.'] I-LAWYER\n",
      "['V'] I-LAWYER\n",
      "['in'] I-LAWYER\n",
      "['od'] I-LAWYER\n",
      "['Ċ'] I-LAWYER\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n",
      "['Ġ'] O\n"
     ]
    }
   ],
   "source": [
    "for (token, label) in zip(test[0]['input_ids'], preds[0]):\n",
    "    print(tokenizer.convert_ids_to_tokens([token]), labels[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 40\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>'] O O\n",
      "['<PREAMBLE>'] O O\n",
      "['Ġ'] O O\n",
      "['ĊĊ'] O O\n",
      "['Ġ('] O O\n",
      "['Ġ7'] O O\n",
      "['Ġ)'] O O\n",
      "['ĠOn'] O O\n",
      "['Ġspecific'] O O\n",
      "['Ġquery'] O O\n",
      "['Ġby'] O O\n",
      "['Ġthe'] O O\n",
      "['ĠBench'] O O\n",
      "['Ġabout'] O O\n",
      "['Ġan'] O O\n",
      "['Ġentry'] O O\n",
      "['Ġof'] O O\n",
      "['ĠRs'] O O\n",
      "['Ġ.'] O O\n",
      "['Ġ1'] O O\n",
      "[','] O O\n",
      "['31'] O O\n",
      "[','] O O\n",
      "['37'] O O\n",
      "[','] O O\n",
      "['500'] O O\n",
      "['Ġon'] O O\n",
      "['Ġdeposit'] O O\n",
      "['Ġside'] O O\n",
      "['Ġof'] O O\n",
      "['ĠHong'] B-ORG B-ORG\n",
      "['k'] B-ORG I-ORG\n",
      "['ong'] B-ORG I-ORG\n",
      "['ĠBank'] I-ORG I-ORG\n",
      "['Ġaccount'] O O\n",
      "['Ġof'] O O\n",
      "['Ġwhich'] O O\n",
      "['Ġa'] O O\n",
      "['Ġphoto'] O O\n",
      "['Ġcopy'] O O\n",
      "['Ġis'] O O\n",
      "['Ġappearing'] O O\n",
      "['Ġat'] O O\n",
      "['Ġp'] O O\n",
      "['.'] O O\n",
      "['Ġ40'] O O\n",
      "['Ġof'] O O\n",
      "['Ġass'] O O\n",
      "['essee'] O O\n",
      "[\"Ġ'\"] O O\n",
      "['s'] O O\n",
      "['Ġpaper'] O O\n",
      "['Ġbook'] O O\n",
      "['Ġ,'] O O\n",
      "['Ġlearned'] O O\n",
      "['Ġauthorised'] O O\n",
      "['Ġrepresentative'] O O\n",
      "['Ġsubmitted'] O O\n",
      "['Ġthat'] O O\n",
      "['Ġit'] O O\n",
      "['Ġwas'] O O\n",
      "['Ġrelated'] O O\n",
      "['Ġto'] O O\n",
      "['Ġloan'] O O\n",
      "['Ġfrom'] O O\n",
      "['Ġbroker'] O O\n",
      "['Ġ,'] O O\n",
      "['ĠRahul'] B-ORG B-ORG\n",
      "['Ġ&'] I-ORG I-ORG\n",
      "['ĠCo'] I-ORG I-ORG\n",
      "['.'] I-ORG I-ORG\n",
      "['Ġon'] O O\n",
      "['Ġthe'] O O\n",
      "['Ġbasis'] O O\n",
      "['Ġof'] O O\n",
      "['Ġhis'] O O\n",
      "['Ġsubmission'] O O\n",
      "['Ġa'] O O\n",
      "['Ġnecessary'] O O\n",
      "['Ġmark'] O O\n",
      "['Ġis'] O O\n",
      "['Ġput'] O O\n",
      "['Ġby'] O O\n",
      "['Ġus'] O O\n",
      "['Ġon'] O O\n",
      "['Ġthat'] O O\n",
      "['Ġphoto'] O O\n",
      "['Ġcopy'] O O\n",
      "['Ġ.'] O O\n",
      "['</s>'] O O\n"
     ]
    }
   ],
   "source": [
    "def train_tokenize(row, idx):\n",
    "    # Add special token for document type\n",
    "    is_preamble = True\n",
    "    if is_preamble:\n",
    "        row['tokens'].insert(0, '<PREAMBLE>')\n",
    "    else:\n",
    "        row['tokens'].insert(0, '<JUDGEMENT>')\n",
    "    row['tags'].insert(0, 'O')\n",
    "\n",
    "    tokenized = tokenizer(row['tokens'], truncation=True, is_split_into_words=True)\n",
    "    aligned_labels = []\n",
    "    last_i = None\n",
    "    for i in tokenized.word_ids():\n",
    "        if i is None:\n",
    "            aligned_labels.append(-100)\n",
    "            continue\n",
    "        \n",
    "        aligned_label = row['tags'][i] # Find the appropriate label index\n",
    "        if not i == last_i:\n",
    "            aligned_labels.append(labels.index(aligned_label))\n",
    "        else:\n",
    "            aligned_labels.append(labels.index(aligned_label.replace('B', 'I')))\n",
    "        last_i = i\n",
    "    tokenized['labels'] = aligned_labels\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = train_tokenize(train[0], 0)\n",
    "train_preds = np.argmax(trainer.predict([train_tokenized])[0], axis=2)[0]\n",
    "for (token, label, real) in zip(train_tokenized['input_ids'], train_preds, train_tokenized['labels']):\n",
    "    print(tokenizer.convert_ids_to_tokens([token]), labels[label], labels[real if real >= 0 else 28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Wa-305', '-', '2007', '\\n         ', '(', 'Purushottam', 'Lal', 'Vs', 'The', 'State', 'Of', 'Madhya', 'Pradesh', ')', '\\n\\n\\n', '15', '-', '10', '-', '2015', '\\n    ', 'High', 'Court', 'Of', 'Madhya', 'Pradesh', 'Principal', '\\n              ', 'Seat', 'At', 'Jabalpur', '\\n                 ', 'Writ', 'Appeal', 'No.305/2007', '\\n                 ', 'Purushottam', 'Lal', 'and', 'others', '\\n                             ', 'Vs', '.', '\\n                   ', 'State', 'of', 'M.P.', '&', 'Others', '\\n', 'Present', ':', 'Honâ\\x80\\x99ble', 'Shri', 'Rajendra', 'Menon', ',', 'J.', '&', '\\n', \"Hon'ble\", 'Shri', 'C.', 'V.', 'Sirpurkar', ',', 'J.', '\\n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '\\n', 'Shri', 'Vivek', 'Tankha', ',', 'learned', 'Senior', 'Counsel', 'with', 'Shri', 'Varun', 'K.', '\\n', 'Chopra', ',', 'Shri', 'Akshay', 'Sapre', ',', 'for', 'the', 'appellants', '.', '\\n', 'Shri', 'Swapnil', 'Ganguly', ',', 'learned', 'Govt', '.', 'Adv', '.', ',', 'for', 'the', 'respondents', '\\n', 'State', '.', '\\n', 'Ku', '.', 'Anjali', 'Banerjee', ',', 'learned', 'counsel', 'for', 'M.P.', 'Housing', 'Board', '.', '\\n', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '_', '\\n                       ', 'Judgment', '\\n'], 'tags': ['O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(train[-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'In The High Court Of Kerala At Ernakulam\\n\\n                                       Present\\n\\n                    The Honourable Mrs. Justice M.R.Anitha\\n\\n           Wednesday, The 10Th Day Of June 2020 / 20Th Jyaishta, 1942\\n\\n                          Crl.Rev.Pet.No.767 Of 2012\\n\\n    Crmp 1176/2011 Dated 16-03-2012 Of Judicial Magistrate Of First Class ,\\n                                 Kunnamkulam\\n\\nRevision Petitioner/Complainant\\n                A. Rajesh Aged 35 Years,\\n                S/O.Raman Nair, Ammasom Veettil, Punnayurkulam Village, Cherayi\\n                Desom, Andathodu P.O., Chavakkad, Thrissur District, Pin\\n                679564.\\n                By Advs.\\n                Dr.V.N.Sankarjee\\n                Sri.V.N.Madhusudanan\\n                Sri.S.Sidhardhan\\n                Smt.R.Udaya Jyothi\\n                Sri.M.M.Vinod\\n                Smt.M.Suseela\\n                Sri.Sudhakaran V.\\n                Smt.Arya Balachandran\\n                Smt. Keerthi B. Chandran\\n\\nRespondents/State And Accused 1 To 6\\n\\n       1        State Of Kerala\\n                Represented By The Public Prosecutor, High Court Of Kerala,\\n                Ernakulam.\\n       2        Sajin Sasi\\n                Sub Inspector Of Police, Vadakkekkad, Thrissur District, Pin\\n                679564.\\n\\n       3        Surendran\\n                Police Constable, Vadakkekad, Pin 679564.\\n\\n       4        Joshi,Assistant Sub Inspector Of Police, Vadakkekkad, Pin\\n                679564.\\n\\n       5        Dinesan, Police Constable, Vadakkekad, Pin 679564.\\n\\n\\n\\n       6        Stephen\\n                Police Constable, Vadakkekad, Pin 679564.\\n\\n                By Adv. Sri.K.K.Dheerendrakrishnan\\n                By Adv. Sri.S.Rajeev\\n                Sr.Pp- Sri. M.S.Breez\\n\\n      This Criminal Revision Petition Having Been Finally Heard On 26.5.2020,\\nThe Court On 10.06.2020, The Court On The Same Day Passed The Following:\\n Crl.R.P.767/2012\\n                                            2\\n\\n                                 Order\\n', 'meta': 'criminal_kerala_high_court preamble https://indiankanoon.org/doc/122996227/', 'input_ids': [0, 50265, 96, 20, 755, 837, 1525, 14558, 497, 42833, 677, 922, 424, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 17356, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 20, 28237, 868, 3801, 4, 1659, 256, 4, 500, 4, 4688, 3432, 102, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 307, 6, 20, 158, 11329, 1053, 1525, 502, 2760, 1589, 291, 11329, 344, 2636, 1173, 4349, 6, 27784, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 5309, 462, 4, 38494, 4, 28970, 4, 3084, 4, 38504, 1525, 1125, 50140, 1437, 1437, 1437, 5309, 6195, 365, 5067, 73, 22748, 211, 1070, 545, 12, 3933, 12, 14517, 1525, 20502, 3771, 18104, 1525, 1234, 4210, 2156, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 15941, 8697, 330, 922, 424, 50118, 50118, 38494, 10699, 40505, 254, 73, 14721, 21306, 927, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 83, 4, 3288, 4891, 83, 4462, 1718, 10426, 6, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 208, 73, 673, 4, 500, 7243, 234, 2456, 6, 1918, 13738, 1075, 10978, 2645, 718, 6, 14687, 282, 857, 710, 330, 922, 424, 6389, 6, 13881, 857, 118, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4762, 1075, 6, 178, 2681, 1630, 257, 221, 4, 673, 482, 732, 1469, 677, 330, 625, 6, 25181, 3006, 710, 1384, 6, 12009, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 231, 36346, 4027, 4, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 870, 17638, 29, 4, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 925, 4, 846, 4, 487, 4, 104, 3153, 271, 11650, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 6604, 4, 846, 4, 487, 4, 21067, 25134, 1906, 23935, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 6604, 4, 104, 4, 104, 808, 9635, 4134, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4966, 90, 4, 500, 4, 791, 1208, 102, 344, 219, 6157, 118, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 6604, 4, 448, 4, 448, 4, 846, 179, 1630, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4966, 90, 4, 448, 4, 104, 3698, 6658, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 6604, 4, 104, 1906, 28832, 14119, 468, 4, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4966, 90, 4, 250, 1506, 102, 4317, 1488, 463, 3917, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4966, 90, 4, 3350, 254, 212, 118, 163, 4, 12598, 3917, 50118, 50118, 44036, 2832, 4189, 73, 13360, 178, 5438, 6199, 112, 598, 231, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 112, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 331, 1525, 14558, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 27893, 196, 870, 20, 1909, 13992, 6, 755, 837, 1525, 14558, 6, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 42833, 677, 922, 424, 4, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 132, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 208, 1176, 179, 208, 8209, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 4052, 12412, 1525, 522, 6, 468, 625, 677, 1071, 27762, 625, 6, 25181, 3006, 710, 1384, 6, 12009, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 231, 36346, 4027, 4, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 155, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 9136, 1187, 3917, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 522, 25218, 6, 468, 625, 677, 1071, 330, 625, 6, 12009, 231, 36346, 4027, 4, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 204, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 3533, 118, 6, 46184, 4052, 12412, 1525, 522, 6, 468, 625, 677, 1071, 27762, 625, 6, 12009, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 231, 36346, 4027, 4, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 195, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 211, 3141, 260, 6, 522, 25218, 6, 468, 625, 677, 1071, 330, 625, 6, 12009, 231, 36346, 4027, 4, 50140, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 231, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 3259, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 522, 25218, 6, 468, 625, 677, 1071, 330, 625, 6, 12009, 231, 36346, 4027, 4, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 870, 17638, 4, 6604, 4, 530, 4, 530, 4, 495, 700, 2816, 1187, 9418, 14884, 10197, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 870, 17638, 4, 6604, 4, 104, 4, 500, 1176, 1942, 705, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 8482, 4, 510, 642, 12, 6604, 4, 256, 4, 104, 4, 387, 5314, 329, 50140, 1437, 1437, 1437, 1437, 1437, 152, 10203, 45323, 40505, 5365, 30857, 3347, 30689, 374, 973, 4, 245, 4, 24837, 6, 50118, 133, 837, 374, 158, 4, 4124, 4, 24837, 6, 20, 837, 374, 20, 17754, 1053, 45498, 20, 3515, 35, 50118, 5309, 462, 4, 500, 4, 510, 4, 38504, 73, 14517, 50118, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 132, 50140, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 1437, 9729, 50118, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "for item in test:\n",
    "    if len(item['input_ids']) > 512:\n",
    "        long = item\n",
    "        print(item)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['V', 'in', 'od', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(long['input_ids'][500:512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
