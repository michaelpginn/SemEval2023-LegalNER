{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e0734489-80f2-4311-9344-13ba2b059381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/added_tokens.json. We won't load it.\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/vocab.json\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/merges.txt\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/tokenizer.json\n",
      "loading file None\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/special_tokens_map.json\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmpfu_2kt_9/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['\\n\\n', '(', '7', ')', 'On', 'specific', 'query', 'by', 'the', 'Bench', 'about', 'an', 'entry', 'of', 'Rs', '.', '1,31,37,500', 'on', 'deposit', 'side', 'of', 'Hongkong', 'Bank', 'account', 'of', 'which', 'a', 'photo', 'copy', 'is', 'appearing', 'at', 'p.', '40', 'of', 'assessee', \"'s\", 'paper', 'book', ',', 'learned', 'authorised', 'representative', 'submitted', 'that', 'it', 'was', 'related', 'to', 'loan', 'from', 'broker', ',', 'Rahul', '&', 'Co.', 'on', 'the', 'basis', 'of', 'his', 'submission', 'a', 'necessary', 'mark', 'is', 'put', 'by', 'us', 'on', 'that', 'photo', 'copy', '.'], 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "['B-CASE_NUMBER', 'B-COURT', 'B-DATE', 'B-GPE', 'B-JUDGE', 'B-LAWYER', 'B-ORG', 'B-OTHER_PERSON', 'B-PETITIONER', 'B-PRECEDENT', 'B-PROVISION', 'B-RESPONDENT', 'B-STATUTE', 'B-WITNESS', 'I-CASE_NUMBER', 'I-COURT', 'I-DATE', 'I-GPE', 'I-JUDGE', 'I-LAWYER', 'I-ORG', 'I-OTHER_PERSON', 'I-PETITIONER', 'I-PRECEDENT', 'I-PROVISION', 'I-RESPONDENT', 'I-STATUTE', 'I-WITNESS', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Load data from spaCy format\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_data(spacy_file='training/data/train.spacy'):\n",
    "    doc_bin = DocBin().from_disk(spacy_file)\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "    docs = doc_bin.get_docs(nlp.vocab)\n",
    "    \n",
    "    all_sents = []\n",
    "    all_labels = set()\n",
    "    skip_chars = ['=']\n",
    "    for i, doc in enumerate(docs):\n",
    "        new_sent = {'tokens': [token.text for token in doc if token.text not in skip_chars],\n",
    "                    'tags': [token.ent_iob_ + (\"-\" + token.ent_type_ if token.ent_type_ else '') for token in doc if token.text not in skip_chars]}\n",
    "        \n",
    "        if spacy_file == 'training/data/dev.spacy' and i == 15:\n",
    "            new_sent['tags'] = ['B-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "        all_sents.append(new_sent)\n",
    "        [all_labels.add(tag) for tag in new_sent['tags']]\n",
    "    return Dataset.from_list(all_sents), sorted(list(all_labels))\n",
    "\n",
    "train, labels = load_data()\n",
    "print(train[0])\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "def292df-3865-4dc9-b180-6337e3be283b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/config.json from cache at /Users/milesper/.cache/huggingface/transformers/8fb343045f345372593fb03cc8edd339757b9b5fa164bfd4a071c33d81906423.81e683d6e4f3240a109c9cfd0742b01d443acc6fc2b120b20796fb85ddf0f393\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"nlpaueb/legal-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/vocab.txt from cache at /Users/milesper/.cache/huggingface/transformers/a745361317c5abff68001c41dcd59cb03a59590cd41b528cee213f748d6a5383.1369ebdf147c85eaf03a9d4c661bb19c36a776431d4ef10065ac548d08d4368c\n",
      "loading file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/special_tokens_map.json from cache at /Users/milesper/.cache/huggingface/transformers/1ec992491addc8a43e9196bea3ccaf98f3958ce101f111ba9096813cf1ab493c.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "loading file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/milesper/.cache/huggingface/transformers/f795d176d70a24ac32d30bc7b2ee9a6743e562fb84ea9fa94aae5082ebb18cf6.76ea01b4b85ac16e2cec55c398cba7a943d89ab21dfdd973f6630a152e4b9aed\n",
      "loading configuration file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/config.json from cache at /Users/milesper/.cache/huggingface/transformers/8fb343045f345372593fb03cc8edd339757b9b5fa164bfd4a071c33d81906423.81e683d6e4f3240a109c9cfd0742b01d443acc6fc2b120b20796fb85ddf0f393\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"nlpaueb/legal-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/nlpaueb/legal-bert-base-uncased/resolve/main/config.json from cache at /Users/milesper/.cache/huggingface/transformers/8fb343045f345372593fb03cc8edd339757b9b5fa164bfd4a071c33d81906423.81e683d6e4f3240a109c9cfd0742b01d443acc6fc2b120b20796fb85ddf0f393\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"nlpaueb/legal-bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 207, 231, 248, 1325, 213, 392, 1409, 223, 248, 896, 153, 172, 229, 783, 217, 2816, 213, 267, 210, 2606, 212, 2417, 189, 238, 411, 1949, 653, 189, 231, 268, 2497, 645, 653, 189, 117, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 28, 28, 28, 28, 28, 28, 28, 28, 10, 24, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, -100]}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
    "\n",
    "def tokenize(row):\n",
    "    tokenized = tokenizer(row['tokens'], truncation=True, is_split_into_words=True)\n",
    "    aligned_labels = [-100 if i is None else labels.index(row['tags'][i]) for i in tokenized.word_ids()]\n",
    "    tokenized['labels'] = aligned_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenize(train[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "66c542f7-19a0-441c-a45d-c4a0c6f4c616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e93d5a8d46145c29d7b7bed7a6bb174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10995 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"/var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/config.json\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Didn't find file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/added_tokens.json. We won't load it.\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/vocab.json\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/merges.txt\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/tokenizer.json\n",
      "loading file None\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/special_tokens_map.json\n",
      "loading file /var/folders/1y/v6m6qm2d3p52jn2sv6smp5cr0000gn/T/tmp1y2xisax/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f10e3440c1433dbac23510985f5e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1074 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "'' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [216], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mmap(tokenize)\n\u001b[1;32m      4\u001b[0m dev, _ \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/data/dev.spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[43mdev\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dev\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:2585\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2582\u001b[0m disable_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:585\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:552\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    546\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    550\u001b[0m }\n\u001b[1;32m    551\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 552\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:2967\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   2966\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m-> 2967\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2968\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   2969\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:2865\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   2864\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 2865\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2867\u001b[0m     \u001b[38;5;66;03m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2868\u001b[0m     update_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[38;5;241m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/datasets/arrow_dataset.py:2545\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2541\u001b[0m decorated_item \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2542\u001b[0m     Example(item, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m Batch(item, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m   2543\u001b[0m )\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 2545\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecorated_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, LazyDict) \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn [215], line 7\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(row):\n\u001b[1;32m      6\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m tokenizer(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     aligned_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mindex(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m][i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokenized\u001b[38;5;241m.\u001b[39mword_ids()]\n\u001b[1;32m      8\u001b[0m     tokenized[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aligned_labels\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n",
      "Cell \u001b[0;32mIn [215], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(row):\n\u001b[1;32m      6\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m tokenizer(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     aligned_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tokenized\u001b[38;5;241m.\u001b[39mword_ids()]\n\u001b[1;32m      8\u001b[0m     tokenized[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m aligned_labels\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n",
      "\u001b[0;31mValueError\u001b[0m: '' is not in list"
     ]
    }
   ],
   "source": [
    "train = train.map(tokenize)\n",
    "\n",
    "\n",
    "dev, _ = load_data('training/data/dev.spacy')\n",
    "dev = dev.map(tokenize)\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "94026c1f-2429-46ab-96ab-c585a62137f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained('nlpaueb/legal-bert-base-uncased', num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6907d691-532f-45bf-a2b3-501caaf9cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=16\n",
    "args = TrainingArguments(\n",
    "    f\"output\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=3,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14ef01c2-f1ce-40d9-a2e1-5d74c8f72295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca9e0da4-dc81-41ac-8248-db2c282ec193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5aedd6a-5d34-4dcb-9fbc-70cc609a810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea3b0649-17c7-428c-851d-cdb9bc86a001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "897cd7cb-46ee-4952-8349-b19081022d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7f554404-ee4e-4bae-a9a7-0142c211097f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['High', 'Court', 'Of', 'Judicature', 'At', 'Allahabad', '\\n \\n \\n\\n                                                                             ', 'A.F.R.', '\\n \\n                                                 \\t          ', 'Reserved', 'on', '07.10.2021', '\\n \\n\\t\\t\\t\\t\\t\\t          ', 'Delivered', 'on', '13.12.2021', '\\n \\n\\n \\n\\n \\n', 'Case', ':', '-', 'Writ', '-', 'C', 'No', '.', '-', '59863', 'of', '2015', '\\n \\n', 'Petitioner', ':', '-', 'Sun', 'Tower', 'Residents', 'Welfare', 'Association', '\\n \\n', 'Respondent', ':', '-', 'Ghaziabad', 'Development', 'Authority', 'through', 'its', 'Vice', 'Chairman', 'and', '2', 'Others', '\\n \\n', 'Counsel', 'for', 'Petitioner', ':', '-', 'Prashant', ',', 'Abhijeet', 'Mukherji', ',', 'Prashant', ',', 'S.K.', 'Pal', '\\n \\n', 'Counsel', 'for', 'Respondent', ':', '-', 'Ram', 'Bilas', 'Yadav', ',', 'Anoop', 'Tivedi', ',', 'Anoop', 'Trivedi', '(', 'Senior', 'Adv.),Himanshu', 'Tyagi', ',', 'Kartikeya', 'Saran', ',', 'Rahul', 'Agarwal', ',', 'Rakesh', 'Kumar', 'Singh', ',', 'S.Shekhar', ',', 'Vrindavan', 'Mishra', '\\n \\n\\n \\n                       ', 'Connected', 'with', '\\n \\n\\n \\n', 'Case', ':', '-', 'Writ', '-', 'C', 'No', '.', '-', '11072', 'of', '2017', '\\n \\n', 'Petitioner', ':', '-', 'Sun', 'Tower', 'Residents', 'Welfare', 'Association', 'through', 'Vice', 'President', '\\n \\n', 'Respondent', ':', '-', 'Ghaziabad', 'Development', 'Authority', 'through', 'its', 'Vice', 'Chairman', 'and', '2', 'Others', '\\n \\n', 'Counsel', 'for', 'Petitioner', ':', '-', 'Abhijeet', 'Mukherji', ',', 'S.', 'Shekhar', ',', 'Vrindavan', 'Mishra', '\\n \\n', 'Counsel', 'for', 'Respondent', ':', '-', 'C.S.C.,Ram', 'Bilas', 'Yadav', ',', 'Tarun', 'Agrawal', '\\n \\n\\n \\n', \"Hon'ble\", 'Arvind', 'Kumar', 'Mishra', '-', 'I', ',', 'J.', '\\n\\n\\n', \"Hon'ble\", 'Naveen', 'Srivastava', ',', 'J.', '\\n\\n', '[', 'Per', 'Arvind', 'Kumar', 'Mishra', '-', 'I', ',', 'J.', ']', '\\n \\n']\n"
     ]
    }
   ],
   "source": [
    "print(dev[15]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d4a66-2539-46e9-b517-6d1294d72dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6ca95069-73c6-42e6-976c-cb4043e14511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(len(dev[15]['tokens']) * ['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "24d00c31-2866-457c-ac67-73feb41fe8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('High', 'B-COURT'), ('Court', 'I-COURT'), ('Of', 'I-COURT'), ('Judicature', 'I-COURT'), ('At', 'I-COURT'), ('Allahabad', 'I-COURT'), ('\\n \\n \\n\\n                                                                             ', 'O'), ('A.F.R.', 'O'), ('\\n \\n                                                 \\t          ', 'O'), ('Reserved', 'O'), ('on', 'O'), ('07.10.2021', 'O'), ('\\n \\n\\t\\t\\t\\t\\t\\t          ', 'O'), ('Delivered', 'O'), ('on', 'O'), ('13.12.2021', 'O'), ('\\n \\n\\n \\n\\n \\n', 'O'), ('Case', 'O'), (':', 'O'), ('-', 'O'), ('Writ', 'O'), ('-', 'O'), ('C', 'O'), ('No', 'O'), ('.', 'O'), ('-', 'O'), ('59863', 'O'), ('of', 'O'), ('2015', 'O'), ('\\n \\n', 'O'), ('Petitioner', 'O'), (':', 'O'), ('-', 'O'), ('Sun', 'B-PETITIONER'), ('Tower', 'I-PETITIONER'), ('Residents', 'I-PETITIONER'), ('Welfare', 'I-PETITIONER'), ('Association', 'I-PETITIONER'), ('\\n \\n', 'O'), ('Respondent', 'O'), (':', 'O'), ('-', 'O'), ('Ghaziabad', 'B-RESPONDENT'), ('Development', 'I-RESPONDENT'), ('Authority', 'I-RESPONDENT'), ('through', 'O'), ('its', 'O'), ('Vice', 'O'), ('Chairman', 'O'), ('and', 'O'), ('2', 'O'), ('Others', 'O'), ('\\n \\n', 'O'), ('Counsel', 'O'), ('for', 'O'), ('Petitioner', 'O'), (':', 'O'), ('-', 'O'), ('Prashant', 'B-LAWYER'), (',', 'O'), ('Abhijeet', 'B-LAWYER'), ('Mukherji', 'I-LAWYER'), (',', 'O'), ('Prashant', 'B-LAWYER'), (',', 'O'), ('S.K.', 'B-LAWYER'), ('Pal', 'I-LAWYER'), ('\\n \\n', 'O'), ('Counsel', 'O'), ('for', 'O'), ('Respondent', 'O'), (':', 'O'), ('-', 'O'), ('Ram', 'B-LAWYER'), ('Bilas', 'I-LAWYER'), ('Yadav', 'I-LAWYER'), (',', 'O'), ('Anoop', 'B-LAWYER'), ('Tivedi', 'I-LAWYER'), (',', 'O'), ('Anoop', 'B-LAWYER'), ('Trivedi', 'I-LAWYER'), ('(', 'O'), ('Senior', 'O'), ('Adv.),Himanshu', 'B-LAWYER'), ('Tyagi', 'I-LAWYER'), (',', 'O'), ('Kartikeya', 'B-LAWYER'), ('Saran', 'I-LAWYER'), (',', 'O'), ('Rahul', 'B-LAWYER'), ('Agarwal', 'I-LAWYER'), (',', 'O'), ('Rakesh', 'B-LAWYER'), ('Kumar', 'I-LAWYER'), ('Singh', 'I-LAWYER'), (',', 'O'), ('S.Shekhar', 'B-LAWYER'), (',', 'O'), ('Vrindavan', 'B-LAWYER'), ('Mishra', 'I-LAWYER'), ('\\n \\n\\n \\n                       ', 'O'), ('Connected', 'O'), ('with', 'O'), ('\\n \\n\\n \\n', 'O'), ('Case', 'O'), (':', 'O'), ('-', 'O'), ('Writ', 'O'), ('-', 'O'), ('C', 'O'), ('No', 'O'), ('.', 'O'), ('-', 'O'), ('11072', 'O'), ('of', 'O'), ('2017', 'O'), ('\\n \\n', 'O'), ('Petitioner', 'O'), (':', 'O'), ('-', 'O'), ('Sun', 'B-PETITIONER'), ('Tower', 'I-PETITIONER'), ('Residents', 'I-PETITIONER'), ('Welfare', 'I-PETITIONER'), ('Association', 'I-PETITIONER'), ('through', 'O'), ('Vice', 'O'), ('President', 'O'), ('\\n \\n', 'O'), ('Respondent', 'O'), (':', 'O'), ('-', 'O'), ('Ghaziabad', 'B-RESPONDENT'), ('Development', 'I-RESPONDENT'), ('Authority', 'I-RESPONDENT'), ('through', 'O'), ('its', 'O'), ('Vice', 'O'), ('Chairman', 'O'), ('and', 'O'), ('2', 'O'), ('Others', 'O'), ('\\n \\n', 'O'), ('Counsel', 'O'), ('for', 'O'), ('Petitioner', 'O'), (':', 'O'), ('-', 'O'), ('Abhijeet', 'B-LAWYER'), ('Mukherji', 'I-LAWYER'), (',', 'O'), ('S.', 'B-LAWYER'), ('Shekhar', 'I-LAWYER'), (',', 'O'), ('Vrindavan', 'B-LAWYER'), ('Mishra', 'I-LAWYER'), ('\\n \\n', 'O'), ('Counsel', 'O'), ('for', 'O'), ('Respondent', 'O'), (':', 'O'), ('-', 'O'), ('C.S.C.,Ram', 'B-LAWYER'), ('Bilas', 'I-LAWYER'), ('Yadav', 'I-LAWYER'), (',', 'O'), ('Tarun', 'B-LAWYER'), ('Agrawal', 'I-LAWYER'), ('\\n \\n\\n \\n', 'O'), (\"Hon'ble\", 'O'), ('Arvind', 'B-JUDGE'), ('Kumar', 'I-JUDGE'), ('Mishra', 'I-JUDGE'), ('-', 'O'), ('I', 'O'), (',', 'O'), ('J.', 'O'), ('\\n\\n\\n', 'O'), (\"Hon'ble\", 'O'), ('Naveen', 'B-JUDGE'), ('Srivastava', 'I-JUDGE'), (',', 'O'), ('J.', 'O'), ('\\n\\n', 'O'), ('[', 'O'), ('Per', 'O'), ('Arvind', 'B-JUDGE'), ('Kumar', 'I-JUDGE'), ('Mishra', 'I-JUDGE'), ('-', 'O'), ('I', 'O'), (',', 'O'), ('J.', 'O'), (']', 'O'), ('\\n \\n', 'O')]\n"
     ]
    }
   ],
   "source": [
    "missing_row = ['B-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'I-COURT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'I-PETITIONER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-RESPONDENT', 'I-RESPONDENT', 'I-RESPONDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAWYER', 'I-LAWYER', 'I-LAWYER', 'O', 'B-LAWYER', 'I-LAWYER', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'B-JUDGE', 'I-JUDGE', 'I-JUDGE', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "print(list(zip(dev[15]['tokens'], missing_row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f19fafe1-261a-4b62-9728-4dcda9ee00f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5fa0bcaf-ec69-49d9-a3f5-089caa631173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for row in dev:\n",
    "    if row['tags'][0] == '':\n",
    "        print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "742bd8ac-8e7d-444b-9ae9-03d5271f2c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a, b \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a, b = (1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b4a0ec4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import train_sentence_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d877ae-b355-4b96-b398-ce535732ac6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
