{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aecfe2f4-fccc-4934-a5a1-bff602ab36be",
   "metadata": {},
   "source": [
    "# Document Classifier Model\n",
    "We are building a document classifer for the legal texts so we can classify whether a sentence is from the preamble or judgement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f244600-0c73-44d5-8ae3-1f94ff11dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c17807cb-93b2-41e7-8fee-f7b8e6c058f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In The High Court Of Kerala At Ernakulam\\n\\nCrl Mc No. 1622 of 2006()\\n\\n\\n1. T.R.Ajayan, S/O. O.Raman,\\n                      ...  Petitioner\\n\\n                        Vs\\n\\n\\n\\n1. M.Ravindran,\\n                       ...       Respondent\\n\\n2. Mrs. Nirmala Dinesh, W/O. Dinesh,\\n\\n                For Petitioner  :Sri.A.Kumar\\n\\n                For Respondent  :Smt.M.K.Pushpalatha\\n\\nThe Hon'ble Mr. Justice P.R.Raman\\nThe Hon'ble Mr. Justice V.K.Mohanan\\n\\n Dated :07/01/2008\\n\\n O R D E R\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preamble = pd.read_json(\"NER_TRAIN/NER_TRAIN_PREAMBLE.json\")\n",
    "preamble_texts = [item['text'] for item in preamble['data']]\n",
    "preamble_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e591a90e-1cc1-42b3-ae3c-d9cadee2e2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n(7) On specific query by the Bench about an entry of Rs. 1,31,37,500 on deposit side of Hongkong Bank account of which a photo copy is appearing at p. 40 of assessee's paper book, learned authorised representative submitted that it was related to loan from broker, Rahul & Co. on the basis of his submission a necessary mark is put by us on that photo copy.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judgement = pd.read_json(\"NER_TRAIN/NER_TRAIN_JUDGEMENT.json\")\n",
    "judgement_texts = [item['text'] for item in judgement['data']]\n",
    "judgement_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc5157ca-3e7f-4f30-a358-07d4197d7808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   111,   204,   112,   222,   599, 18380,   218,   207,  7396,\n",
       "           802,   237,  1003,   210,   162,   189,   117,   198,   115,   415,\n",
       "           115,   984,   115,  1272,   222,  1101,  2336,   210,  4413, 11904,\n",
       "           177,   429,   366,   210,   229,   145,  5732,  1081,   223,  4973,\n",
       "           236,   160,   117,   673,   210,  2351,   175,   175,   110,   163,\n",
       "          1513,  2301,   115,  6532,  1837,   900,   642,   216,   233,   246,\n",
       "           497,   211,   413,   238,  2757,   115,  3679,  6416,   182,   109,\n",
       "           535,   117,   222,   207,   421,   210,   275,  1927,   145,   369,\n",
       "          1233,   223,  1206,   218,  1143,   222,   216,  5732,  1081,   117,\n",
       "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from typing import List\n",
    "\n",
    "class BatchTokenizer:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        \n",
    "    def __call__(self, sentence: List[str]):\n",
    "        return self.tokenizer(\n",
    "            sentence,\n",
    "            padding='max_length', \n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "    \n",
    "tokenizer = BatchTokenizer()\n",
    "tokenizer([judgement_texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8e386ed3-e0eb-432f-a56f-a84901afc821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "# Create labels for each of the sentences\n",
    "all_texts = preamble_texts + judgement_texts\n",
    "all_labels = [1] * len(preamble_texts) + [0] * len(judgement_texts)\n",
    "\n",
    "# Randomize order\n",
    "combined = list(zip(all_texts, all_labels))\n",
    "random.shuffle(combined)\n",
    "all_texts[:], all_labels[:] = zip(*combined)\n",
    "\n",
    "print(all_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7246b78a-4ac0-4680-b477-a5328cf75dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,   300,   261,  ...,     0,     0,     0],\n",
       "        [  101,   207, 15959,  ...,     0,     0,     0],\n",
       "        [  101,   373,   207,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   435,   207,  ...,     0,     0,     0],\n",
       "        [  101,   226,   968,  ...,     0,     0,     0],\n",
       "        [  101,   213,   207,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "        \n",
    "batch_size = 64\n",
    "\n",
    "train_input_batches = [b for b in chunk(all_texts, batch_size)]\n",
    "train_input_batches = [tokenizer(batch) for batch in train_input_batches]\n",
    "\n",
    "train_input_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ebec3c77-cd77-43a8-b8f2-671da1219bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch labels as well\n",
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    return torch.FloatTensor([int(l) for l in labels]).to(device)\n",
    "\n",
    "train_label_batches = [encode_labels(b) for b in chunk(all_labels, batch_size)]\n",
    "train_label_batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "867d21e2-237e-47f2-999c-437d9c417bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceBinaryClassifier(torch.nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.legal_bert = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "        self.bert_hidden_dim = self.legal_bert.config.hidden_size\n",
    "        for param in self.legal_bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.hidden_layer = torch.nn.Linear(self.bert_hidden_dim, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, sentences) -> torch.Tensor:\n",
    "        src = self.legal_bert(**sentences).pooler_output\n",
    "        src = self.relu(self.hidden_layer(src))\n",
    "        out = self.classifier(src)\n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "def predict(model, sents):\n",
    "    return model(sents) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "04330b5e-c01c-4a47-9800-8ad2671ff8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def training_loop(num_epochs, train_sentences, train_labels, dev_sentences, dev_labels, optimizer, model):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.BCELoss()\n",
    "    batches = list(zip(train_sentences, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for sents, labels in tqdm(batches):\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(sents).squeeze(1)\n",
    "            loss = loss_func(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        print(\"Evaluating dev\")\n",
    "        dev_preds = []\n",
    "        dev_labels = []\n",
    "        for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            dev_preds.extend(pred)\n",
    "            dev_labels.extend(list(labels.numpy()))\n",
    "        accuracy = sum(dev_preds == dev_labels) / len(dev_labels)\n",
    "        print(f\"Dev Acc: {accuracy}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be6233d7-5d60-4bc8-835d-97514bc63bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble_texts_dev = [item['text'] for item in pd.read_json(\"NER_DEV/NER_DEV_PREAMBLE.json\")['data']]\n",
    "judgement_texts_dev = [item['text'] for item in pd.read_json(\"NER_DEV/NER_DEV_JUDGEMENT.json\")['data']]\n",
    "all_texts_dev = preamble_texts_dev + judgement_texts_dev\n",
    "all_labels_dev = [1] * len(preamble_texts_dev) + [0] * len(judgement_texts_dev)\n",
    "dev_sents_batches = [tokenizer(b) for b in chunk(all_texts_dev, batch_size)]\n",
    "dev_labels_batches = [encode_labels(b) for b in chunk(all_labels_dev, batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff7bbd-2c64-42d4-aeef-bf84dc5450d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e662d33331274696a5508e15c0a231d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceBinaryClassifier(hidden_size=128).to(device)\n",
    "\n",
    "training_loop(\n",
    "    num_epochs=10,\n",
    "    train_sentences=train_input_batches,\n",
    "    train_labels=train_label_batches,\n",
    "    dev_sentences=dev_sents_batches,\n",
    "    dev_labels=dev_labels_batches,\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78b79e-0191-4243-8588-252db323ed87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
